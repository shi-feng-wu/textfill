{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f3a12c-b7e4-46a2-b1cb-3555ecd1d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to .\\tokenized_text\\train_2023 Amazon Shareholder Letter.txt.json and .\\tokenized_text\\test_2023 Amazon Shareholder Letter.txt.json\n",
      "Data saved to .\\tokenized_text\\train_A Brief History of Time.txt.json and .\\tokenized_text\\test_A Brief History of Time.txt.json\n",
      "Data saved to .\\tokenized_text\\train_Adventure Of HuckleBerry.txt.json and .\\tokenized_text\\test_Adventure Of HuckleBerry.txt.json\n",
      "Data saved to .\\tokenized_text\\train_Christian Science.txt.json and .\\tokenized_text\\test_Christian Science.txt.json\n",
      "Data saved to .\\tokenized_text\\train_Complete Works of Jane Austen.txt.json and .\\tokenized_text\\test_Complete Works of Jane Austen.txt.json\n",
      "Data saved to .\\tokenized_text\\train_The Bible.txt.json and .\\tokenized_text\\test_The Bible.txt.json\n",
      "Data saved to .\\tokenized_text\\train_The Great Gatsby.txt.json and .\\tokenized_text\\test_The Great Gatsby.txt.json\n",
      "Data saved to .\\tokenized_text\\train_The Lightning Thief.txt.json and .\\tokenized_text\\test_The Lightning Thief.txt.json\n",
      "Data saved to .\\tokenized_text\\train_Tom Sawyer.txt.json and .\\tokenized_text\\test_Tom Sawyer.txt.json\n"
     ]
    }
   ],
   "source": [
    "# tokenizes all the txt files in the input folder and saves them to the output folder\n",
    "import re\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def pad_tokens(token_lists, desired_length, pad_token_id):\n",
    "    padded_lists = [tokens + [pad_token_id] * (desired_length - len(tokens)) for tokens in token_lists]\n",
    "    return padded_lists\n",
    "\n",
    "def tokenize_text_file(text, tokenizer, chunk_size=512, test_split=0.1):\n",
    "    # tokenize text and split into chunks\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    \n",
    "    # split into test and train sets\n",
    "    np.random.shuffle(chunks)\n",
    "    split_index = int(len(chunks) * (1 - test_split))\n",
    "    train_chunks = chunks[:split_index]\n",
    "    test_chunks = chunks[split_index:]\n",
    "    \n",
    "    # pad chunks to make sure all are 512 long\n",
    "    train_chunks = pad_tokens(train_chunks, chunk_size, tokenizer.pad_token_id)\n",
    "    test_chunks = pad_tokens(test_chunks, chunk_size, tokenizer.pad_token_id)\n",
    "    \n",
    "    return train_chunks, test_chunks\n",
    "    \n",
    "input_dir = r'.\\clean_text'\n",
    "output_dir = r'.\\tokenized_text'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # tokenize and split the cleaned text file\n",
    "        train_data, test_data = tokenize_text_file(text, tokenizer)\n",
    "\n",
    "        # save the tokenized data\n",
    "        train_data_file = os.path.join(output_dir, f'train_{filename}.json')\n",
    "        test_data_file = os.path.join(output_dir, f'test_{filename}.json')\n",
    "        with open(train_data_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_data, f)\n",
    "        with open(test_data_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_data, f)\n",
    "        print(f\"Data saved to {train_data_file} and {test_data_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2671a-2c8a-434f-861a-2bb073090dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
